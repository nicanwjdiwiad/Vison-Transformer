{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1nXi4cfXf5WVjAGz9dohPTQr60CO2nv16",
      "authorship_tag": "ABX9TyNJsYAgIsFl+5ktnjp/u+Li",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicanwjdiwiad/Vison-Transformer/blob/main/ViTipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CPZfxbGjcF3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae30806-a786-4363-df9d-e407fb29fa6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#第一步：把图片变为embedding"
      ],
      "metadata": {
        "id": "qNKKuqLf3E29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DNN版本实现"
      ],
      "metadata": {
        "id": "eL_YlY9DweWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "\n",
        "#将图片转为embedding\n",
        "#weight是把token(patch)线性变换转为embedding的权重 patch*W=embedding\n",
        "def image2emb_naive(image, patch_size, weight):\n",
        "  #image_size = batchsize*channel*h*w 像素点个数\n",
        "  #使用拿出卷积区域的api，把每次滑动的小区域的输入拿出来\n",
        "  #因为图像没有重叠的部分，所以说滑动卷积核步长stride=块边长\n",
        "  patch = f.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n",
        "  # print(patch.shape)\n",
        "  #patch.Size([1, 4, 48])即有1个4*48大小的矩阵，48个像素表示一个patch，4个patch表示一张图片\n",
        "  #reason: 1:image数量\n",
        "  #    4:一个图片中patch数量\n",
        "  #    48:一个patch中像素数量=patch面积*channel\n",
        "\n",
        "  patch_embedding = patch @ weight\n",
        "  return patch_embedding\n",
        "\n",
        "\n",
        "#test code 4 image2emb(patch的size)\n",
        "batch_size,inchannel,image_h,image_w=1,3,8,8\n",
        "\n",
        "patch_size=4\n",
        "\n",
        "model_dim=8\n",
        "\n",
        "patch_xiangsu_number = patch_size*patch_size*inchannel\n",
        "\n",
        "image = torch.randn(batch_size,inchannel,image_h,image_w)\n",
        "\n",
        "weight = torch.randn(patch_xiangsu_number,model_dim)\n",
        "\n",
        "# print(weight.shape)  #weight.Size([48, 8])\n",
        "\n",
        "patch_embedding_naive = image2emb_naive(image, patch_size, weight)\n",
        "\n",
        "print(patch_embedding_naive.shape) #torch.Size([1, 4, 8]),可以明显看出一个patch一个embedding,且向量长度为d_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDdLHuwonAiZ",
        "outputId": "e279d154-69b4-4cc4-a3a9-d5df4c8a8ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN版本实现"
      ],
      "metadata": {
        "id": "KP1PqnkDwh2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "\n",
        "def image2emb_conv(image, kernel, stride):\n",
        "  #1.卷积，得到输出尺寸batchsize*channel*h*w\n",
        "  conv_output=f.conv2d(image, kernel, stride=stride)\n",
        "  bs,oc,oh,ow=conv_output.shape\n",
        "  #2.把卷积结果拉直\n",
        "  patch_embedding=conv_output.reshape((bs,oc,oh*ow)).transpose(-1,-2)\n",
        "  return patch_embedding\n",
        "\n",
        "#test code 4 image2emb(patch的size)\n",
        "batch_size,inchannel,image_h,image_w=1,3,8,8\n",
        "\n",
        "patch_size=4\n",
        "\n",
        "model_dim=8\n",
        "\n",
        "max_num_token = 16 #序列最长时token数,位置编码行数上界\n",
        "\n",
        "num_classes = 10 #最后目标分类数量\n",
        "\n",
        "label = torch.randn(batch_size,10) #随机生成10个数模拟真实类标签\n",
        "\n",
        "patch_xiangsu_number = patch_size*patch_size*inchannel\n",
        "\n",
        "image = torch.randn(batch_size,inchannel,image_h,image_w)\n",
        "\n",
        "weight = torch.randn(patch_xiangsu_number,model_dim) #这里把模型维度视为out_channel.patch_xiangsu_number是卷积核面积*IN_channel\n",
        "\n",
        "# print(weight.shape)  #weight.Size([48, 8])\n",
        "\n",
        "kernel=weight.transpose(0,1).reshape((model_dim,inchannel,patch_size,patch_size)) #卷积核维度oc*ic*kh*kw\n",
        "patch_embedding_conv = image2emb_conv(image, kernel, patch_size)\n",
        "print(patch_embedding_conv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly-Wky_dwkxZ",
        "outputId": "37e79117-7ff8-40d4-e0b4-11dc8307d1a1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#第二步：增加一个Query(cls)去指导embedding的表示(最后是分类任务)\n",
        "CLS token embedding"
      ],
      "metadata": {
        "id": "gZ9Q2bw23JoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token_embedding = torch.randn(batch_size,1,model_dim,requires_grad=True) #有梯度表示可训练的embedding\n",
        "print(cls_token_embedding.shape) #torch.Size([1, 1, 8])\n",
        "#将指导emb与原emb concat\n",
        "visual_token_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=1)#dim=1表示在位置上去cat\n",
        "print(visual_token_embedding.shape) #torch.Size([1, 5, 8])\n",
        "#这里可以理解为cls视为图像的分类特征，此时图像有5个patch，所以加载dim=1维度"
      ],
      "metadata": {
        "id": "9lsQqNqe3WFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020fe064-4d34-4673-a57e-555ed943e1ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 8])\n",
            "torch.Size([1, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#第三步加位置编码"
      ],
      "metadata": {
        "id": "aQUWUxDn4B8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "position_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\n",
        "seq_len = visual_token_embedding.shape[1] #这里是5,表示5个patch需要位置编码\n",
        "position_embedding=torch.tile(position_embedding_table[:seq_len],[visual_token_embedding.shape[0],1,1]) #表示dim=0是visual_token_embedding.shape[0]=1，dim=1==position_embedding_table[:seq_len][0],dim=2==position_embedding_table[:seq_len][1]\n",
        "visual_token_embedding += position_embedding"
      ],
      "metadata": {
        "id": "69AGTrQM4EYL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#第四步：将embedding送入transformer encoder"
      ],
      "metadata": {
        "id": "ZXeQl8Kn5Bzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim,nhead=8) #8个attention头\n",
        "transformer_encoder=nn.TransformerEncoder(encoder_layer,num_layers=6)  #6层encoder\n",
        "encoder_output = transformer_encoder(visual_token_embedding)\n",
        "print(encoder_output.shape)"
      ],
      "metadata": {
        "id": "VvvEdIkC5Gyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f034e372-85d4-4a32-a9fe-4fd008589f2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#与cls做交叉熵，做分类"
      ],
      "metadata": {
        "id": "IdAbiNFm5wU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token_output = encoder_output[:,0,:]\n",
        "print(cls_token_output.shape) #torch.Size([1, 8]) 将patch个数维度删掉，仅考虑模型维度和批量大小\n",
        "linear_layer = nn.Linear(model_dim,num_classes)\n",
        "logits = linear_layer(cls_token_output)\n",
        "loss_fn = nn.CrossEntropyLoss()  #封装了softmax和交叉熵损失函数\n",
        "loss = loss_fn(logits_probs,label)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "pJ_zzRSn52SL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6afb18e-a048-46d6-a65c-19901fbc63f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8])\n",
            "tensor(-11.7735, grad_fn=<DivBackward1>)\n"
          ]
        }
      ]
    }
  ]
}